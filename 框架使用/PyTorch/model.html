<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>PyTorch 模型 | 编程笔记</title>
    <meta name="generator" content="VuePress 1.8.2">
    <script data-ad-client="ca-pub-4932639067711253" async="true" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <meta name="description" content="JustSong 的编程笔记">
    
    <link rel="preload" href="/assets/css/0.styles.f24f2770.css" as="style"><link rel="preload" href="/assets/js/app.0a73e14a.js" as="script"><link rel="preload" href="/assets/js/2.0d6745a2.js" as="script"><link rel="preload" href="/assets/js/42.7426b5d7.js" as="script"><link rel="prefetch" href="/assets/js/10.04a7093c.js"><link rel="prefetch" href="/assets/js/11.99c7d178.js"><link rel="prefetch" href="/assets/js/12.f751b69e.js"><link rel="prefetch" href="/assets/js/13.a12ff563.js"><link rel="prefetch" href="/assets/js/14.f9bb9188.js"><link rel="prefetch" href="/assets/js/15.29345cfd.js"><link rel="prefetch" href="/assets/js/16.1b7c89b3.js"><link rel="prefetch" href="/assets/js/17.f2e5b814.js"><link rel="prefetch" href="/assets/js/18.6b9ac9c1.js"><link rel="prefetch" href="/assets/js/19.cf7dc984.js"><link rel="prefetch" href="/assets/js/20.494e77f9.js"><link rel="prefetch" href="/assets/js/21.ff920d53.js"><link rel="prefetch" href="/assets/js/22.a2f2cf0f.js"><link rel="prefetch" href="/assets/js/23.9907be2c.js"><link rel="prefetch" href="/assets/js/24.eacc5b93.js"><link rel="prefetch" href="/assets/js/25.48c0f461.js"><link rel="prefetch" href="/assets/js/26.59605d23.js"><link rel="prefetch" href="/assets/js/27.429003aa.js"><link rel="prefetch" href="/assets/js/28.141533e4.js"><link rel="prefetch" href="/assets/js/29.61038db6.js"><link rel="prefetch" href="/assets/js/3.31d87108.js"><link rel="prefetch" href="/assets/js/30.749dfcc1.js"><link rel="prefetch" href="/assets/js/31.2f1ac4e1.js"><link rel="prefetch" href="/assets/js/32.6f80f4a3.js"><link rel="prefetch" href="/assets/js/33.2b3032c4.js"><link rel="prefetch" href="/assets/js/34.11195481.js"><link rel="prefetch" href="/assets/js/35.ff00053c.js"><link rel="prefetch" href="/assets/js/36.3afc8c4f.js"><link rel="prefetch" href="/assets/js/37.e81724fe.js"><link rel="prefetch" href="/assets/js/38.44cd6b7e.js"><link rel="prefetch" href="/assets/js/39.8d5ee136.js"><link rel="prefetch" href="/assets/js/4.95d34a0c.js"><link rel="prefetch" href="/assets/js/40.b29fc4b6.js"><link rel="prefetch" href="/assets/js/41.853a75bd.js"><link rel="prefetch" href="/assets/js/43.7bc0e4f4.js"><link rel="prefetch" href="/assets/js/44.c7d90a1a.js"><link rel="prefetch" href="/assets/js/45.70b686b5.js"><link rel="prefetch" href="/assets/js/46.4c3c0d15.js"><link rel="prefetch" href="/assets/js/47.4d495e1a.js"><link rel="prefetch" href="/assets/js/48.d0ae25ec.js"><link rel="prefetch" href="/assets/js/49.e2faf696.js"><link rel="prefetch" href="/assets/js/5.c3a96cc4.js"><link rel="prefetch" href="/assets/js/50.779b80dd.js"><link rel="prefetch" href="/assets/js/51.ba657455.js"><link rel="prefetch" href="/assets/js/52.c9751d56.js"><link rel="prefetch" href="/assets/js/53.5ad721e6.js"><link rel="prefetch" href="/assets/js/54.dfd48d29.js"><link rel="prefetch" href="/assets/js/55.c93e4f07.js"><link rel="prefetch" href="/assets/js/56.7f1363bd.js"><link rel="prefetch" href="/assets/js/57.829f6b96.js"><link rel="prefetch" href="/assets/js/58.ad7f5f49.js"><link rel="prefetch" href="/assets/js/59.a91ddf3f.js"><link rel="prefetch" href="/assets/js/6.e3a0505b.js"><link rel="prefetch" href="/assets/js/60.89737234.js"><link rel="prefetch" href="/assets/js/61.fc90c71f.js"><link rel="prefetch" href="/assets/js/62.96d607f1.js"><link rel="prefetch" href="/assets/js/63.5493abe3.js"><link rel="prefetch" href="/assets/js/64.3a7f3c39.js"><link rel="prefetch" href="/assets/js/7.4eded3f6.js"><link rel="prefetch" href="/assets/js/8.2b0ea8ce.js"><link rel="prefetch" href="/assets/js/9.805f6267.js">
    <link rel="stylesheet" href="/assets/css/0.styles.f24f2770.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">编程笔记</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/编程语言/" class="nav-link">
  语言
</a></div><div class="nav-item"><a href="/框架使用/" class="nav-link">
  框架
</a></div><div class="nav-item"><a href="/工具使用/" class="nav-link">
  工具
</a></div><div class="nav-item"><a href="/算法总结/" class="nav-link">
  算法
</a></div><div class="nav-item"><a href="/Linux 系统/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="https://iamazing.cn" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/songquanpeng/cs-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/编程语言/" class="nav-link">
  语言
</a></div><div class="nav-item"><a href="/框架使用/" class="nav-link">
  框架
</a></div><div class="nav-item"><a href="/工具使用/" class="nav-link">
  工具
</a></div><div class="nav-item"><a href="/算法总结/" class="nav-link">
  算法
</a></div><div class="nav-item"><a href="/Linux 系统/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="https://iamazing.cn" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/songquanpeng/cs-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Linux 系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>工具使用</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>框架使用</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Flask</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>Py Torch</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/PyTorch/" aria-current="page" class="sidebar-link">PyTorch 概述</a></li><li><a href="/框架使用/PyTorch/data.html" class="sidebar-link">PyTorch 数据处理</a></li><li><a href="/框架使用/PyTorch/gpu.html" class="sidebar-link">使用 GPU 加速计算</a></li><li><a href="/框架使用/PyTorch/model.html" class="active sidebar-link">PyTorch 模型</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#模型的构建" class="sidebar-link">模型的构建</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#使用-nn-module-进行构建" class="sidebar-link">使用 nn.module 进行构建</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#使用-nn-sequelize-进行构建" class="sidebar-link">使用 nn.Sequelize 进行构建</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#使用-nn-modulelist-进行构建" class="sidebar-link">使用 nn.ModuleList() 进行构建</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#使用-nn-moduledict-进行构建" class="sidebar-link">使用 nn.ModuleDict() 进行构建</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#构建自定义网络层" class="sidebar-link">构建自定义网络层</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#常用网络层" class="sidebar-link">常用网络层</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#查看模型结构" class="sidebar-link">查看模型结构</a></li></ul></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#模型的参数" class="sidebar-link">模型的参数</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#对于单个网络层的参数初始化" class="sidebar-link">对于单个网络层的参数初始化</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#对于-nn-module的参数初始化" class="sidebar-link">对于 nn.Module的参数初始化</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#模型参数的共享" class="sidebar-link">模型参数的共享</a></li></ul></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#模型的评估" class="sidebar-link">模型的评估</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#模型的保存与加载" class="sidebar-link">模型的保存与加载</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#保存模型" class="sidebar-link">保存模型</a></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#加载模型" class="sidebar-link">加载模型</a></li></ul></li><li class="sidebar-sub-header"><a href="/框架使用/PyTorch/model.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/框架使用/PyTorch/optimizer.html" class="sidebar-link">PyTorch 优化器</a></li><li><a href="/框架使用/PyTorch/tensor.html" class="sidebar-link">PyTorch Tensor</a></li><li><a href="/框架使用/PyTorch/torchvision.html" class="sidebar-link">TorchVision 相关笔记</a></li><li><a href="/框架使用/PyTorch/trick.html" class="sidebar-link">PyTorch 使用技巧</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Sci Py</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>算法总结</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>编程语言</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="pytorch-模型"><a href="#pytorch-模型" class="header-anchor">#</a> PyTorch 模型</h1> <h2 id="模型的构建"><a href="#模型的构建" class="header-anchor">#</a> 模型的构建</h2> <p>实际上模型层和模型没有本质区别，其对外提供的接口一致。</p> <p><code>nn.module</code> 实现了 <code>__call__</code> 函数，因此我们可以直接通过模型来调用，无须手动调用 <code>forward()</code> 函数。</p> <p>继承自 <code>Module</code> 的类还有：<code>Sequential</code>，<code>ModuleList</code> 以及 <code>ModuleDict</code>。</p> <h3 id="使用-nn-module-进行构建"><a href="#使用-nn-module-进行构建" class="header-anchor">#</a> 使用 <code>nn.module</code> 进行构建</h3> <p>这是通过继承 <code>nn.module</code> 来构造网络。</p> <p>该方式的最大优点是灵活，比如说可以自由地复用网络层。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MyNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MyNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_feature<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> MyNet<span class="token punctuation">(</span>num_feature<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
</code></pre></div><h3 id="使用-nn-sequelize-进行构建"><a href="#使用-nn-sequelize-进行构建" class="header-anchor">#</a> 使用 <code>nn.Sequelize</code> 进行构建</h3> <p>nn.Sequelize 内置了一个有序字典，因此前向传播时模块的执行顺序与加入的顺序相同。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 使用构造函数</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>

<span class="token comment"># 使用列表</span>
layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

<span class="token comment"># 使用有序字典，可以指定网络层名字</span>
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
          <span class="token punctuation">(</span><span class="token string">'linear'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token string">'linear'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
          <span class="token punctuation">(</span><span class="token string">'linear'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 使用 add_module 来进行构建</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;linear1&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;relu1&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;linear2&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;relu2&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;linear3&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;sigmoid&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="使用-nn-modulelist-进行构建"><a href="#使用-nn-modulelist-进行构建" class="header-anchor">#</a> 使用 <code>nn.ModuleList()</code> 进行构建</h3> <div class="language-python extra-class"><pre class="language-python"><code>self<span class="token punctuation">.</span>encode <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>decode <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>encode<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ResBlk<span class="token punctuation">(</span>dim_in<span class="token punctuation">,</span> dim_out<span class="token punctuation">)</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>decode<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> AdainResBlk<span class="token punctuation">(</span>dim_out<span class="token punctuation">,</span> dim_in<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>与 <code>nn.Sequelize</code> 的区别：<code>nn.Sequential</code> 是一个 module，其有 forward 函数，因此可以拿来直接输入。</p> <p><code>nn.ModuleList()</code> 相当于一个列表，其中的网络层之间没有联系，不保证次序。</p> <p>不同于直接使用列表的方式，ModuleList 中的所有模块参数会被自动加入到网络中，可以被优化器发现和训练。</p> <h3 id="使用-nn-moduledict-进行构建"><a href="#使用-nn-moduledict-进行构建" class="header-anchor">#</a> 使用 <code>nn.ModuleDict()</code> 进行构建</h3> <p>与 <code>ModuleList</code> 类似，同样没有实现 <code>forward</code> 函数，不过这里换成了字典：</p> <div class="language-python extra-class"><pre class="language-python"><code>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">'linear'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">'act'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 可以像字典那样添加元素</span>
net<span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="构建自定义网络层"><a href="#构建自定义网络层" class="header-anchor">#</a> 构建自定义网络层</h3> <p>虽说模型和模型层没有本质区别，都是 <code>Module</code>，但是对于模型层，构建的时候我们要注意参数的声明和使用。</p> <p>为什么构建模型时不用管呢？因为模型里用的都是现有的模型层，其参数已经声明好了（可能也已经初始化好了）。</p> <p>注意有的层不需要参数，自然也不用管。</p> <p>模型参数的类型可以为：</p> <ol><li><code>nn.Parameter</code>：单个参数，通过参数名访问。</li> <li><code>nn.ParameterList</code>：参数列表，通过索引访问。</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>self<span class="token punctuation">.</span>params <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><ol start="3"><li><code>nn.ParameterDict</code>：参数字典，通过 key 访问。</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>self<span class="token punctuation">.</span>params <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
                <span class="token string">'linear1'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">'linear2'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre></div><p>至于参数的使用，无非是与输入等做一些计算。</p> <h3 id="常用网络层"><a href="#常用网络层" class="header-anchor">#</a> 常用网络层</h3> <div class="language-python extra-class"><pre class="language-python"><code>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span>
nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="查看模型结构"><a href="#查看模型结构" class="header-anchor">#</a> 查看模型结构</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><p>查看每一层的输出是否正常：</p> <div class="language-python extra-class"><pre class="language-python"><code>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> layer <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> <span class="token string">' output shape:\t'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre></div><h2 id="模型的参数"><a href="#模型的参数" class="header-anchor">#</a> 模型的参数</h2> <p><code>Module</code> 类实现了两个相关的函数：</p> <ol><li><code>parameters()</code>：返回参数迭代器。</li> <li><code>named_parameters()</code>：返回 (名字, 参数) 元组迭代器，名字即参数的变量名。</li></ol> <p>参数的类型为 <code>torch.nn.Parameter</code>，其是 Tensor 的子类，特殊之处在于其内的值会自动加入到参数列表。</p> <h3 id="对于单个网络层的参数初始化"><a href="#对于单个网络层的参数初始化" class="header-anchor">#</a> 对于单个网络层的参数初始化</h3> <h4 id="直接初始化为指定值"><a href="#直接初始化为指定值" class="header-anchor">#</a> 直接初始化为指定值</h4> <div class="language-python extra-class"><pre class="language-python"><code>module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre></div><h4 id="使用-torch-nn-init-中的方法"><a href="#使用-torch-nn-init-中的方法" class="header-anchor">#</a> 使用 <code>torch.nn.init</code> 中的方法</h4> <div class="language-python extra-class"><pre class="language-python"><code>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>conv1<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>norm_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="对于-nn-module的参数初始化"><a href="#对于-nn-module的参数初始化" class="header-anchor">#</a> 对于 <code>nn.Module</code>的参数初始化</h3> <p>包括 <code>nn.Sequential</code>。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># 使用内置的初始化方法</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">'weight'</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
        init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> <span class="token string">'bias'</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
        init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>


<span class="token comment"># 自定义参数初始化方法</span>
<span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 判断 module 类型</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token comment"># or</span>
        m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    <span class="token comment"># 也可以这样判断 module 类型</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_in'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> module<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
</code></pre></div><p>要注意，要么在 <code>with torch.no_grad()</code> 里进行参数的初始化操作，要么加上 <code>@torch.no_grad()</code> 注解。</p> <p>对于内置的参数初始化函数，其已经做了类似的处理，例如对于 <code>nn.init.xavier_uniform</code>：</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">xavier_uniform_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span> gain<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tensor<span class="token punctuation">:</span>
    fan_in<span class="token punctuation">,</span> fan_out <span class="token operator">=</span> _calculate_fan_in_and_fan_out<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
    std <span class="token operator">=</span> gain <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>fan_in <span class="token operator">+</span> fan_out<span class="token punctuation">)</span><span class="token punctuation">)</span>
    a <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span> <span class="token operator">*</span> std  <span class="token comment"># Calculate uniform bounds from standard deviation</span>
    <span class="token keyword">return</span> _no_grad_uniform_<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> <span class="token operator">-</span>a<span class="token punctuation">,</span> a<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">_no_grad_uniform_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
</code></pre></div><p>当然，如果是直接操作 <code>.data</code> 属性，那 <code>no_grad</code> 不是必须的（因为这样的话本来就不会被自动求导机制记录）。</p> <h3 id="模型参数的共享"><a href="#模型参数的共享" class="header-anchor">#</a> 模型参数的共享</h3> <p>直接使用同一个 <code>Module</code> 实例就好。</p> <p>反向传播时梯度会累加。</p> <h2 id="模型的评估"><a href="#模型的评估" class="header-anchor">#</a> 模型的评估</h2> <ol><li>加载模型：<code>model.load_state_dict(torch.load(&quot;the path of model's pth file&quot;))</code></li> <li>切换到评估模式：<code>model.eval()</code> ；
<ol><li>为什么要手动指明模式？因为有些东西在训练模式和评估模式下表现不同，例如 Batch Normalization，Dropout。</li> <li>具体是如何实现的？调用 <code>eval()</code> 后会将模型的 training 属性置为 false，Batch Normalization layer 或者 Dropout layer 通过读取 training 属性来判断当前模式从而采用不同的行为。</li> <li><strong>要注意，eval 不会影响梯度的计算，只不过不回传更新参数而已，必须另外额外关闭梯度计算</strong></li></ol></li> <li>关闭模型参数的 <code>requires_grad</code>：</li></ol> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">toggle_grad</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> on_or_off<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> on_or_off

<span class="token comment"># 或者使用 torch.no_grad() 装饰器或者 with 代码块。</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    out_data <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre></div><p>之后网络前向传播后不会再进行求导和进行反向传播。</p> <ol start="4"><li>准备相应的特征和标签，注意要指明放到 GPU 的内存里，例如 <code>x = torch.randn(10, 128).cuda()</code>。</li> <li>使用模型进行预测，例如：<code>predict = model(x)</code>。</li> <li>最后如果还要继续训练，记得：
<ol><li>开启模型参数的 <code>requires_grad</code>（如果使用 <code>torch.no_grad()</code>，结束该 with 块之后 <code>requires_grad</code> 会自动恢复成 <code>Ture</code>），</li> <li>并调用 <code>model.train()</code>。</li></ol></li></ol> <h2 id="模型的保存与加载"><a href="#模型的保存与加载" class="header-anchor">#</a> 模型的保存与加载</h2> <p>参数后缀任意，一般为 <code>pt</code> 或 <code>pth</code>。
注意，有时参数的后缀为 <code>ckpt</code>，即 checkpoint 的缩写。</p> <p>两种类型，第一种是通过 <code>state_dict</code>。</p> <p>该方式和 Tensor 的保存与加载没有本质区别。</p> <p>用到两个函数：</p> <ol><li><code>model.state_dict()</code> ：返回一个映射参数名称到对应 Tensor 的字典对象。</li> <li><code>model.load_state_dict(dict)</code> ：接受一个字典，使用其内的值初始化模型参数。</li></ol> <p>自然，只有有可学习的参数（<code>nn.Parameter</code>）的网络层有这两个方法。</p> <p>另外，优化器也有这两个方法，用以存储其状态和所使用的超参数。</p> <p>这里并没有魔法，只是 Tensor 的存储与加载罢了。</p> <p>第二种是通过以下两个函数：</p> <div class="language-python extra-class"><pre class="language-python"><code>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
</code></pre></div><p>这种看起来更加方便，我们取回的时候甚至都不用实现准备模型实例，但是，序列化后的模型与具体的类和目录结构绑定在一起了，
如果后续文件位置或者类发生改变，模型将不可用，因此不推荐这种方式。</p> <p>要注意，加载后的模型默认是训练模式，如果要进行评估，必须手动切换。</p> <h3 id="保存模型"><a href="#保存模型" class="header-anchor">#</a> 保存模型</h3> <ol><li>只保存模型参数：torch.save(net.state_dict(), &quot;./data/net_parameter.pth&quot;)</li> <li>保存完整的模型（可能会由于设备和目录的改变而出问题）：torch.save(net, './data/net_model.pth')</li> <li>注意如果还要继续训练的话，还需要保存 optimizer 的 state_dict。</li></ol> <h3 id="加载模型"><a href="#加载模型" class="header-anchor">#</a> 加载模型</h3> <ol><li>加载模型参数：net_clone.load_state_dict(torch.load(&quot;./data/net_parameter.pth&quot;))</li> <li>加载完整的模型（区别在于这里的返回值直接是模型，我们不需要事先构造模型）：net_loaded = torch.load('./data/net_model.pth')</li> <li>如果还要继续训练,则还需要加载 optimizer 的参数，当然如果只是使用训练好的模型的话就不需要了。</li></ol> <h2 id="参考"><a href="#参考" class="header-anchor">#</a> 参考</h2> <ol><li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.3_linear-regression-pytorch" target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.3_linear-regression-pytorch<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/2" target="_blank" rel="noopener noreferrer">https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.1_model-construction" target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.1_model-construction<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.2_parameters" target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.2_parameters<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.4_custom-layer" target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.4_custom-layer<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol></div> <footer class="page-edit"><div class="edit-link"><a href="https://github.com/songquanpeng/cs-notes/edit/main/docs/框架使用/PyTorch/model.md" target="_blank" rel="noopener noreferrer">编辑本页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">4/13/2021, 11:00:24 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/框架使用/PyTorch/gpu.html" class="prev">
        使用 GPU 加速计算
      </a></span> <span class="next"><a href="/框架使用/PyTorch/optimizer.html">
        PyTorch 优化器
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0a73e14a.js" defer></script><script src="/assets/js/2.0d6745a2.js" defer></script><script src="/assets/js/42.7426b5d7.js" defer></script>
  </body>
</html>
